---
layout: default
---
<header>
    <div class="row justify-content-center pt-5 pb-5 mb-5 section-alt bg-full">
        <div class="col-4">
            <img src="/assets/img/xai_logoText_black_mask.png" alt="XAI logo black" class="img-fluid">
        </div>
    </div>
</header>
<div class="container">
    <div class="row justify-content-center">
        <div class="col-md-10">
            <div class="lead mb-5">
                <em>Imagine that a wealthy friend of yours asks for a vacation credit card to his bank, to discover that
                    the
                    credit he is offered is very low. The bank teller cannot explain why. Your stubborn friend continues
                    his
                    quest for explanation up to the bank executives, to discover that an algorithm lowered his credit
                    score.
                    Why? After a long investigation, it turns out that the reason is: bad credit by the former owner of
                    your
                    friendâ€™s house.
                </em>
            </div>
            <hr>
            <div>
                <p>Black box AI systems for automated decision making, often based on machine learning over (big) data,
                    map
                    a
                    user's features into a class or a score without exposing the reasons why. This is problematic not
                    only
                    for
                    <strong>lack of transparency</strong>, but also for possible <strong>biases inherited by the
                        algorithms</strong> from human prejudices and collection artifacts hidden in the training data,
                    which
                    may lead to unfair or wrong decisions.
                </p>
                <p>
                    XAI project focuses on the urgent open challenge of how to construct meaningful explanations of
                    opaque
                    AI/ML
                    systems, introducing the local-to-global framework for black box explanation, articulated along
                    three
                    lines:
                </p>
                <ul>
                    <li>The language for expressing explanations in terms of expressive logic rules, with statistical
                        and
                        causal
                        interpretation;
                    </li>
                    <li>The inference of local explanations for revealing the decision rationale for a specific case, by
                        auditing the black-box in the vicinity of the target instance;
                    </li>
                    <li>The bottom-up generalization of many local explanations into simple global ones, with algorithms
                        that
                        optimize for quality and comprehensibility.
                    </li>
                </ul>
                <br>
                <h4>Why is it important for society?</h4>
                <p>Trust is crucial in the adoption of AI/ML technologies, due to perceived challenges to human
                    autonomy,
                    and
                    the lack of knowledge about the assumptions, limitations, and capabilities of AI assistants.
                    Building
                    trust
                    in AI models relies on their capacity to reveal their logic in a comprehensible, human-accessible
                    format,
                    allowing them to understand and validate their decision rationale and highlighting possible biases
                    learned
                    by the model.
                </p>
                <p>
                    Trust is crucial in the adoption of AI/ML technologies, due to perceived challenges to human
                    autonomy,
                    and
                    the lack of knowledge about the assumptions, limitations, and capabilities of AI assistants.
                    Building
                    trust
                    in AI models relies on their capacity to reveal their logic in a comprehensible, human-accessible
                    format,
                    allowing them to understand and validate their decision rationale and highlighting possible biases
                    learned
                    by the model.</p>
                <h4>What are the overall objectives?</h4>
                <p>The XAI project faces the challenge of requiring AI to be explainable and understandable in human
                    terms
                    and
                    articulates its research along 5 Research Lines</p>
                <ol>
                    <li>Algorithms to infer local explanations and their generalization to global ones (post-hoc) and
                        algorithms
                        that are transparent by-design;
                    </li>
                    <li>Languages for expressing explanations in terms of logic rules, with statistical and causal
                        interpretation;
                    </li>
                    <li>XAI watchdog platform for sharing experimental dataset and explanation algorithms;</li>
                    <li>A repertoire of case studies aimed at in involving also final users;</li>
                    <li>A framework to study the interplay between XAI and ethical and legal dimensions.</li>
                </ol>
            </div>
            <div>
                <button class="btn">
                    <a href="/research-lines">More info on each research line can be found here</a>
                </button>
            </div>
        </div>
    </div>
</div>